{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import time \n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "from newspaper import Article, ArticleException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to categorize news articles, and to achieve this, we first need to gather data that will later be used to train classification models. To accomplish this task, I utilized the web scraping technique, which involves extracting data from a website.\n",
    "\n",
    "I implemented a scraping function that utilizes the **Selenium** library to initialize a web driver based on the provided _URL_. The function then performs string manipulation to iterate over selected categories for a specified number of pages. The resulting output is a dictionary that contains all the downloaded links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(url_base, n_pages, categories):\n",
    "    ''' \n",
    "    Perform Web-Scraping: \n",
    "    \n",
    "    Args: \n",
    "        url_base: Base URL for the web scraping. This is the starting point for constructing page URLs.\n",
    "        n_pages:  Number of pages to scrape. Specifies how many pages of the website to visit and extract data from.\n",
    "        categories: List of categories to focus on during scraping. These can be used to filter and extract specific information.\n",
    "\n",
    "    Output:\n",
    "        all_links_dict: A dictionary containing URLs categorized by the specified categories. Each key represents a category, and the associated value is a list of URLs \n",
    "        extracted from the specified number of pages for that category.\n",
    "    '''\n",
    "\n",
    "    # Initialize the web driver\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    all_links_dict = {}  # Dictionary to store URLs by category\n",
    "\n",
    "    for category in categories:\n",
    "        all_links = []\n",
    "\n",
    "        for page in range(0, n_pages):\n",
    "            url = f'{url_base}{category}&from={page * 10}&types=article&sort=relevance'\n",
    "            driver.get(url)\n",
    "\n",
    "            # Handle cookies\n",
    "            try:\n",
    "                accept_button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'onetrust-accept-btn-handler')))\n",
    "                accept_button.click()\n",
    "            except:\n",
    "                # Handle the case where the accept button is not found\n",
    "                pass\n",
    "\n",
    "            # Find and collect links\n",
    "            links = driver.find_elements(By.CSS_SELECTOR, '.container_list-images-with-description__link')\n",
    "            for link in links:\n",
    "                href = link.get_attribute('href')\n",
    "                \n",
    "                if href not in all_links:\n",
    "                    all_links.append(href)\n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "        all_links_dict[category] = all_links\n",
    "        print(f'Finished processing {category} category')\n",
    "    \n",
    "    # Close the driver when done\n",
    "    driver.quit()\n",
    "    return all_links_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the function I chosen a search query on CNN's website, a famous source for news and information. \n",
    "The categories I am interested to are instored in the list: **categories_list**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing Politics category\n",
      "Finished processing Business category\n",
      "Finished processing Health category\n",
      "Finished processing Entertainment category\n"
     ]
    }
   ],
   "source": [
    "categories_list = ['Politics', 'Business', 'Health', 'Entertainment']\n",
    "links = scraping('https://www.cnn.com/search?q=', n_pages = 30, categories = categories_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cnn.com/2023/12/30/business/small-businesses-big-retailers-impact/index.html',\n",
       " 'https://www.cnn.com/2022/08/19/economy/worker-shortage-small-business/index.html',\n",
       " 'https://www.cnn.com/2021/07/06/business/money/small-business-retirement/index.html',\n",
       " 'https://www.cnn.com/2022/04/05/success/starting-a-business-retirement-finances/index.html',\n",
       " 'https://www.cnn.com/2023/08/09/business/dentons-dacheng-china-business-split-intl-hnk/index.html',\n",
       " 'https://www.cnn.com/2023/11/21/business/small-business-owners-gear-up-for-the-biggest-shopping-weekend-of-the-holiday-season/index.html',\n",
       " 'https://www.cnn.com/2023/06/13/economy/nfib-small-business-optimism/index.html',\n",
       " 'https://www.cnn.com/cnn-underscored/deals/best-small-business-deals-amazon-prime-day-2023-07-11',\n",
       " 'https://www.cnn.com/cnn-underscored/deals/best-small-business-deals-amazon-prime-day-2023-07-12',\n",
       " 'https://www.cnn.com/2020/04/13/business/businesses-transition-online-trnd/index.html']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['Business']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates through each category and its associated URLs, using the **Article** class from the **newspaper3k** library to download, parse, and extract information from the articles. Specifically it download a summary of the whole text with the method **.summary** It then gather this information, including category, full description, and title, into a DataFrame. The DataFrame is further processed to handle missing values and converted into a dictionary. Finally, this dictionary is saved in a JSON file, and the resulting dictionary is returned.\n",
    "\n",
    "The function is designed to handle exceptions during the downloading and parsing process.\n",
    "If an article cannot be successfully downloaded or parsed, the function logs the error, providing empty values for the respective article's description and title. That makes the function more robust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_news(urls_dict, categories):\n",
    "    '''\n",
    "    Downloads news texts from provided URLs and stores the information.    \n",
    "    \n",
    "    Args:\n",
    "    urls_dict: A dictonary where the keys are the categories and the values are lists of urls.\n",
    "    categories: List of categories.\n",
    "    \n",
    "    Output:\n",
    "    news_text: A dictionary containing downloaded texts per URL in urls_dict.\n",
    "                The dictionary is saved in a JSON file on the computer.\n",
    "    '''\n",
    "    news_texts_list = []\n",
    "\n",
    "    for category, urls in urls_dict.items():\n",
    "        for url in urls:\n",
    "            try:\n",
    "                article = Article(url=url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "                description = article.summary\n",
    "                title = article.title\n",
    "                news_texts_list.append({\n",
    "                    'Category': category,  \n",
    "                    'Full_description': description,\n",
    "                    'Title': title\n",
    "                })\n",
    "            except ArticleException as e:\n",
    "                print(f\"ArticleException: {str(e)}\")\n",
    "                news_texts_list.append({\n",
    "                    'Category': category,\n",
    "                    'Full_description': '',\n",
    "                    'Title':''\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                news_texts_list.append({\n",
    "                    'Category': category,\n",
    "                    'Full_description': '',\n",
    "                    'Title': ''\n",
    "            \n",
    "                })\n",
    "                \n",
    "    news_texts = pd.DataFrame(news_texts_list)\n",
    "    news_texts = news_texts.replace('', np.nan)  \n",
    "    news_texts = news_texts.dropna(subset=['Full_description', 'Title'])\n",
    "    \n",
    "    news_texts = news_texts.to_dict()\n",
    "\n",
    "    json_file_path = 'C:/Users/Lenovo/Desktop/IT coding/IT project/news_trial.json'\n",
    "    # Save the URLs to a JSON file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(news_texts, json_file)\n",
    "        \n",
    "    return news_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArticleException: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/deals/best-small-business-deals-amazon-prime-day-2023-07-11 on URL https://www.cnn.com/cnn-underscored/deals/best-small-business-deals-amazon-prime-day-2023-07-11\n",
      "ArticleException: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/deals/best-small-business-deals-amazon-prime-day-2023-07-12 on URL https://www.cnn.com/cnn-underscored/deals/best-small-business-deals-amazon-prime-day-2023-07-12\n",
      "ArticleException: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.cnn.com/2021/02/01/cnn-underscored/best-home-entertainment-systems on URL https://www.cnn.com/2021/02/01/cnn-underscored/best-home-entertainment-systems\n"
     ]
    }
   ],
   "source": [
    "store_news = store_news(links, categories_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Full_description</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>The case of UNO and AMC 24 Hamilton speaks to ...</td>\n",
       "      <td>What happens to small businesses when big reta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Small business owners across the United States...</td>\n",
       "      <td>America’s small businesses are running out of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>“When you are a small business owner, almost e...</td>\n",
       "      <td>How to navigate retirement as a small business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>For starters, about a third of small businesse...</td>\n",
       "      <td>How to protect your personal finances when you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Hong Kong CNN —Dentons, the world’s biggest la...</td>\n",
       "      <td>Dentons: Global law firm splits off Dacheng, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                   Full_description  \\\n",
       "0  Business  The case of UNO and AMC 24 Hamilton speaks to ...   \n",
       "1  Business  Small business owners across the United States...   \n",
       "2  Business  “When you are a small business owner, almost e...   \n",
       "3  Business  For starters, about a third of small businesse...   \n",
       "4  Business  Hong Kong CNN —Dentons, the world’s biggest la...   \n",
       "\n",
       "                                               Title  \n",
       "0  What happens to small businesses when big reta...  \n",
       "1  America’s small businesses are running out of ...  \n",
       "2  How to navigate retirement as a small business...  \n",
       "3  How to protect your personal finances when you...  \n",
       "4  Dentons: Global law firm splits off Dacheng, i...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(store_news).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
